{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BoostModel.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqI1EPOAY1i7F5HDWBo6wX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feliciahf/data_science_exam/blob/main/BoostModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp-TXoyVK5MF"
      },
      "source": [
        "# From this article: https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv2IKgB9KUw8"
      },
      "source": [
        "from sklearn import model_selection, preprocessing,metrics, \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import xgboost,textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo2xPV72H8_J"
      },
      "source": [
        "#import csv file as dataframe (from GitHub repo)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/feliciahf/data_science_exam/main/hippoCorpusV2.csv'\n",
        "df = pd.read_csv(url, encoding='latin1')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LzzoYNtLH-sH",
        "outputId": "6097ce62-c455-48bc-cf1d-f8111232ff72"
      },
      "source": [
        "# remove columns with uninformative information (AssignmentId, WorkerId, recAgnPairId, recImgPairId)\n",
        "uninformative_cols = [\"AssignmentId\", \"WorkerId\", \"recAgnPairId\", \"recImgPairId\"]\n",
        "df = df.drop(columns=uninformative_cols)\n",
        "df = df[df.memType != 'retold']\n",
        "df "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WorkTimeInSeconds</th>\n",
              "      <th>annotatorAge</th>\n",
              "      <th>annotatorGender</th>\n",
              "      <th>annotatorRace</th>\n",
              "      <th>distracted</th>\n",
              "      <th>draining</th>\n",
              "      <th>frequency</th>\n",
              "      <th>importance</th>\n",
              "      <th>logTimeSinceEvent</th>\n",
              "      <th>mainEvent</th>\n",
              "      <th>memType</th>\n",
              "      <th>mostSurprising</th>\n",
              "      <th>openness</th>\n",
              "      <th>similarity</th>\n",
              "      <th>similarityReason</th>\n",
              "      <th>story</th>\n",
              "      <th>stressful</th>\n",
              "      <th>summary</th>\n",
              "      <th>timeSinceEvent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1641</td>\n",
              "      <td>25.0</td>\n",
              "      <td>man</td>\n",
              "      <td>white</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.499810</td>\n",
              "      <td>attending a show</td>\n",
              "      <td>imagined</td>\n",
              "      <td>when I got concert tickets</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>I've been to a couple concerts, but not many.</td>\n",
              "      <td>Concerts are my most favorite thing, and my bo...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>My boyfriend and I went to a concert together ...</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1245</td>\n",
              "      <td>25.0</td>\n",
              "      <td>woman</td>\n",
              "      <td>white</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.499810</td>\n",
              "      <td>a concert.</td>\n",
              "      <td>recalled</td>\n",
              "      <td>we saw the beautiful sky.</td>\n",
              "      <td>1.000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The day started perfectly, with a great drive ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>My boyfriend and I went to a concert together ...</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1159</td>\n",
              "      <td>35.0</td>\n",
              "      <td>woman</td>\n",
              "      <td>black</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.010635</td>\n",
              "      <td>my sister having her twins a little early</td>\n",
              "      <td>imagined</td>\n",
              "      <td>she went into labor early</td>\n",
              "      <td>0.500</td>\n",
              "      <td>3.0</td>\n",
              "      <td>I am a mother myself</td>\n",
              "      <td>It seems just like yesterday but today makes f...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>My sister gave birth to my twin niece and neph...</td>\n",
              "      <td>150.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>500</td>\n",
              "      <td>30.0</td>\n",
              "      <td>woman</td>\n",
              "      <td>white</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.010635</td>\n",
              "      <td>meeting my twin niece and nephew.</td>\n",
              "      <td>recalled</td>\n",
              "      <td>finding out they were healthy.</td>\n",
              "      <td>1.000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Five months ago, my niece and nephew were born...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>My sister gave birth to my twin niece and neph...</td>\n",
              "      <td>150.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1074</td>\n",
              "      <td>25.0</td>\n",
              "      <td>man</td>\n",
              "      <td>white</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.401197</td>\n",
              "      <td>the consequences of going to burning man</td>\n",
              "      <td>imagined</td>\n",
              "      <td>When I don't answer the phone in case I owe th...</td>\n",
              "      <td>0.250</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Because I also have money problems</td>\n",
              "      <td>About a month ago I went to burning man. I was...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>It is always a journey for me to go to burning...</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6849</th>\n",
              "      <td>926</td>\n",
              "      <td>30.0</td>\n",
              "      <td>woman</td>\n",
              "      <td>other</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.010635</td>\n",
              "      <td>losing and finding a pet.</td>\n",
              "      <td>recalled</td>\n",
              "      <td>the kitten ran into my arms.</td>\n",
              "      <td>0.125</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My dog was diagnosed with lymphoma a year ago ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>My dog, who had lymphoma, was suffering so I h...</td>\n",
              "      <td>150.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6850</th>\n",
              "      <td>3044</td>\n",
              "      <td>18.0</td>\n",
              "      <td>woman</td>\n",
              "      <td>asian</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.345636</td>\n",
              "      <td>about a vacation event worked on</td>\n",
              "      <td>recalled</td>\n",
              "      <td>when i encountered an guy who was really scared</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Over my vacation from my job, I went to Casper...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>On vacation, a side job was taken to plan an e...</td>\n",
              "      <td>570.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6851</th>\n",
              "      <td>1008</td>\n",
              "      <td>35.0</td>\n",
              "      <td>man</td>\n",
              "      <td>asian</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.044522</td>\n",
              "      <td>my nephew's birthday party</td>\n",
              "      <td>recalled</td>\n",
              "      <td>a lot of people got in the pool.</td>\n",
              "      <td>0.500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This event was a birthday party for my nephew....</td>\n",
              "      <td>2.0</td>\n",
              "      <td>This was a birthday party for my nephew that h...</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6852</th>\n",
              "      <td>1462</td>\n",
              "      <td>30.0</td>\n",
              "      <td>man</td>\n",
              "      <td>hisp</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.639057</td>\n",
              "      <td>my cousin's birthday</td>\n",
              "      <td>recalled</td>\n",
              "      <td>my cousin threw a tantrum in the middle of the...</td>\n",
              "      <td>0.500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This event occurred about two weeks ago. I was...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>It was my little cousin's birthday and went to...</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6853</th>\n",
              "      <td>1662</td>\n",
              "      <td>45.0</td>\n",
              "      <td>woman</td>\n",
              "      <td>white</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.891820</td>\n",
              "      <td>building trust in the best interest of a child.</td>\n",
              "      <td>recalled</td>\n",
              "      <td>my daughter agreed for guardianship.</td>\n",
              "      <td>0.750</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Over the past year I have been involved with a...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>My daughter had my granddaughter removed from ...</td>\n",
              "      <td>49.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5535 rows Ã— 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      WorkTimeInSeconds  ...  timeSinceEvent\n",
              "0                  1641  ...            90.0\n",
              "1                  1245  ...            90.0\n",
              "2                  1159  ...           150.0\n",
              "3                   500  ...           150.0\n",
              "4                  1074  ...            30.0\n",
              "...                 ...  ...             ...\n",
              "6849                926  ...           150.0\n",
              "6850               3044  ...           570.0\n",
              "6851               1008  ...            21.0\n",
              "6852               1462  ...            14.0\n",
              "6853               1662  ...            49.0\n",
              "\n",
              "[5535 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYxwpCJ9IAbQ"
      },
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['story'], df['memType'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcIUv3uuLIQd"
      },
      "source": [
        "##Feature enigineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZj9_5-dod8z"
      },
      "source": [
        "##2.1 Count Vectors as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwnBfu7aK2wR"
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(df['story'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZEpzcxnojKR"
      },
      "source": [
        "##2.2 TF-IDF Vectors as features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G26AJ8MyLPyO",
        "outputId": "44bd7885-95f6-47d0-aa22-3e5af605aea3"
      },
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(df['story'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(df['story'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(df['story'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axDMu7RZopdH"
      },
      "source": [
        "##2.3 Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6q9ZySBdyFn",
        "outputId": "28c48c89-1fc1-4572-8989-b8526c3a0152"
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.8)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.7.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.30.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.12.4)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (20.9)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (56.1.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwkVAVDRd0WE"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahvOBLWzd4j6"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlQRFJjQd_9r",
        "outputId": "c485048a-ddce-4a82-dafb-61913bfd102c"
      },
      "source": [
        " link = 'https://drive.google.com/file/d/1yF874Zl9jjL3dQX7FRRg5E9X49VN6kHP/view?usp=sharing' # The shareable link\n",
        " fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '="
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sharing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRjM9ifoeI7o"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1yF874Zl9jjL3dQX7FRRg5E9X49VN6kHP\"})   # the id of our file\n",
        "downloaded.GetContentFile('wiki-news-300d-1M.vec')        # the filename of our file"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtLe2j-uLlC6"
      },
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('wiki-news-300d-1M.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(df['story'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIxE6yPhLzuL"
      },
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(xtrain_count)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = count_vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJySAHefjLR2"
      },
      "source": [
        "##Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9obv3rrrjKBt"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "        predictions =  [int(round(p[0])) for p in predictions]\n",
        "    \n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_aiHl8aMpjp"
      },
      "source": [
        "##The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ7-24mPo--C",
        "outputId": "cb577d69-7434-43b5-c747-4617295a4824"
      },
      "source": [
        "# Extereme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print (\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print (\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xgb, Count Vectors:  0.7174855491329479\n",
            "Xgb, WordLevel TF-IDF:  0.7001445086705202\n",
            "Xgb, CharLevel Vectors:  0.7001445086705202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-YwoOrGpJOj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}